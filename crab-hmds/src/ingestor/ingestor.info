========================
flowchart TD
    subgraph IngestorService["HMDS IngestorService"]
        direction TB
        HF[HistoricalFetcher<br/>async fn fetch_historical()]
        DN[DataNormalizer<br/>OHLCV/Tick unify]
        RS[RealtimeSubscriber<br/>async WS subscribe()]
        DD[Deduplicator<br/>remove overlap]
        QB[DataBuffer / Queue<br/>tokio mpsc / crossbeam]
    end

    SL[Storage Layer<br/>DB / Redis / S3<br/>maintain T_last]
    PL[Processing Layer / SDK<br/>strategy / backtest / unified access]

    HF --> DN
    RS --> DN
    DN --> DD
    DD --> QB
    QB --> SL
    SL --> PL
🔑 图中说明
HistoricalFetcher：批量拉取历史数据

RealtimeSubscriber：WebSocket 实时订阅数据

DataNormalizer：统一数据结构和时间戳

Deduplicator：去重，保证历史+实时无重叠

DataBuffer / Queue：异步缓冲，解耦采集和存储

Storage Layer：持久化 + 更新最新时间戳 T_last

Processing Layer / SDK：下游策略访问接口

=============================
实时数据量小、需要低延迟落盘，但又不能频繁写库浪费资源；同时还要不断 回溯历史数据维护完整性。我们可以把这个场景拆成两部分来设计解决方案。

1️⃣ 实时数据落盘优化
问题：

Tick 或分钟 K 线实时产生，数量小，但下游策略需要“尽快可用”。

直接每条写数据库会造成频繁 IO，非常浪费资源。

解决方案：缓冲 + 批量落盘 + 推送通知
方案 A：异步批量落盘 + 触发器通知

DataBuffer 采用异步队列（Rust async / crossbeam / tokio mpsc）

批量落盘策略：

阈值触发：达到 N 条数据或间隔 T 秒落盘

例如：N = 50条 或 T = 1秒

触发下游更新：

数据写库后，通过消息队列或事件通知下游服务，保证最新数据可用

// pseudo Rust async
async fn data_writer(mut rx: Receiver<DataEvent>, db: &Database) {
    let mut batch = Vec::new();
    let flush_interval = tokio::time::interval(Duration::from_secs(1));

    loop {
        tokio::select! {
            Some(event) = rx.recv() => {
                batch.push(event);
                if batch.len() >= 50 {
                    db.write_batch(&batch).await;
                    batch.clear();
                    notify_downstream().await;
                }
            },
            _ = flush_interval.tick() => {
                if !batch.is_empty() {
                    db.write_batch(&batch).await;
                    batch.clear();
                    notify_downstream().await;
                }
            }
        }
    }
}

方案 B：内存 + 缓存 + 增量落盘

Redis / 本地缓存 存储最新数据

下游服务可以实时从缓存读取最新 K 线 / Tick

数据库批量落盘可延迟，例如每 1 秒或每 1 分钟写一次

✅ 优点：

保证下游策略获取最新数据

降低数据库写入压力

2️⃣ 历史数据回溯与维护
问题：

历史数据可能存在缺失、错误或交易所更新

需要周期性回溯、补齐历史数据

解决方案：
A. 周期性批量任务

定期拉取过去 N 天的数据（例如每日 1 次，补过去 30 天）

比较数据库已有数据，发现缺失区间再拉取

B. DAG 调度

任务调度器（Airflow / Rust cron / tokio timers）

每个 symbol 单独调度，支持失败重试

可以区分：

历史回溯任务：低优先级、批量拉取

实时采集任务：高优先级、低延迟

C. 与实时采集结合

历史回溯可能会与实时数据重叠：

先拉历史数据

使用 Deduplicator 去掉与实时数据重叠部分

最终统一写入缓存和数据库

3️⃣ 数据落盘与下游服务访问策略
层级	存储方式	更新策略	下游访问
最新数据	Redis / in-memory buffer	实时写入 / 小批量 flush	下游直接读缓存，低延迟
历史数据	时序数据库 / Parquet	批量落盘 / 定期回溯	回测或全量分析读取 DB
实时数据落盘	DB 批量写 + flush_interval	保证不丢最新数据	下游策略可通过缓存或 DB 获取增量数据
4️⃣ Rust HMDS 模块设计思路
struct IngestorService {
    historical: HistoricalFetcher,
    realtime: RealtimeSubscriber,
    deduplicator: Deduplicator,
    buffer: tokio::sync::mpsc::Sender<DataEvent>,
    cache: Arc<Mutex<LatestDataCache>>, // 最新数据缓存
}

impl IngestorService {
    async fn start(&self) {
        // 历史数据回溯
        tokio::spawn(self.run_historical_backfill());

        // 实时数据采集
        tokio::spawn(self.run_realtime_subscription());

        // 数据写入
        tokio::spawn(self.run_data_writer());
    }
}


LatestDataCache 保存最近 N 条最新数据，供下游策略快速访问

run_data_writer 批量落盘 + flush_interval + notify downstream

run_historical_backfill 定期回溯历史数据，保证数据完整性

🔑 总结

实时数据落盘：

不每条写数据库，而是 小批量+定时 flush

使用缓存保证下游获取最新数据

历史数据维护：

定期回溯 + 去重

异步任务和 DAG 调度

衔接策略：

Deduplicator 去重

缓存 + 数据库双管齐下

保证下游服务 低延迟可用


======================================最终设计架构===================================
单服务混合架构，同时支持 实时 pipeline 和 历史 DAG 回溯，并保证 数据完整性 + 实时性。我先画出 总体模块架构和数据流关系。

📦 总体模块设计（一个服务）
+--------------------------------------------------------+
|               Quant Market Data Service               |
|--------------------------------------------------------|
| 1. IngestorService                                     |
|    - 管理启动/停止                                     |
|    - 协调历史 DAG + 实时 Pipeline                     |
|                                                        |
|  +----------------------+      +--------------------+ |
|  | HistoricalBackfill   | ---> | Deduplicator       | |
|  | (DAG Scheduler)      |      | - 去重历史+实时    | |
|  +----------------------+      +--------------------+ |
|           ^                          ^                |
|           |                          |                |
|   +-------------------+       +--------------------+ |
|   | HistoricalFetcher  |       | RealtimeSubscriber | |
|   | - 拉历史数据        |       | - WS/Tick流         | |
|   +-------------------+       +--------------------+ |
|                                                        |
|  2. DataBuffer / Queue                                  |
|     - crossbeam / tokio mpsc                            |
|     - 解耦 pipeline + 存储                               |
|                                                        |
|  3. Storage Layer                                       |
|     - DB / Redis / S3                                   |
|     - 写入 + 更新 T_last                                 |
|                                                        |
|  4 +--------------------------+
         | Processing Layer / SDK   |
         | - strategy / backtest    |
         | - unified access         |
         | - 策略/回测/统一访问         |
         +--------------------------+|                             |
|
+--------------------------------------------------------+

🔄 数据流说明

历史 DAG 回溯路径

HistoricalBackfill (定时 DAG)
   -> HistoricalFetcher (API/S3)
   -> Deduplicator
   -> DataBuffer
   -> Storage Layer


批量拉取历史数据

去重 + 幂等写入

更新 T_last

实时 Pipeline 路径

RealtimeSubscriber (WS/Tick)
   -> Deduplicator
   -> DataBuffer / Queue
   -> Storage Layer
   -> Processing Layer / SDK


异步流消费

解耦存储和策略层

与历史数据共享 Deduplicator + Buffer，保证时间线连续

DataBuffer / Queue

统一缓冲层

历史回溯数据和实时数据都通过同一个队列进入存储层

下游处理层统一消费，无需关心来源

Storage Layer

存储历史 + 实时数据

维护 T_last

为策略/回测/分析提供统一访问接口

⚙️ 核心设计思路

单服务：历史回溯和实时订阅共存，节省部署成本

统一 Deduplicator + Buffer：保证历史 + 实时数据时间线连续

DAG 定时任务 + pipeline 流处理：历史批量和实时事件两条逻辑清晰分离

Storage 层统一：保证数据一致性和下游统一接口

模块解耦：未来可拆分成独立服务

你现在的 IngestorService 就像一个 数据调度总控，把 历史补齐 + 实时增量 串起来了，并且通过去重和缓冲保证了数据质量和稳定性。

接下来最值得做的就是：

动态调整订阅（更灵活）

合并器/校验器（保证历史+实时无缝拼接）

持久化/分发（落地到数据库或消息队列，服务策略引擎）